{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "Lab2_Project_preparation_TextProcessing.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V_jT5xRDZDY9"
      },
      "source": [
        "# Ensemble learning from theory to practice\n",
        "Instructors : Parantapa Goswami and Myriam Tami"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GNW-5MGUZDZA"
      },
      "source": [
        "## Lab 2: Text Mining\n",
        "\n",
        "Text mining is the process of automatically extracting \"high-quality\" information from text. High-quality information is typically derived by transforming the free (unstructured) text in documents and databases into normalized, structured data suitable for analysis or to drive machine learning (ML) algorithms.\n",
        "\n",
        "Text mining identifies facts, relationships and assertions that would otherwise remain buried in the mass of textual big data. Once extracted, this information is converted into a structured form that can be further analyzed, or presented directly using clustered HTML tables, mind maps, charts, etc. Text mining employs a variety of methodologies to process the text, one of the most important of these being Natural Language Processing (NLP).\n",
        "\n",
        "Typical text mining applications include:\n",
        "- Text classification\n",
        "    - e.g. spam email detection\n",
        "- Text clustering\n",
        "    - e.g. document retrieval, topic extraction\n",
        "- Sentiment analysis\n",
        "    - e.g. detecting the emotions like \"angry\", \"sad\", and \"happy\" in a customer message\n",
        "- Named entity recognition, etc.\n",
        "    - e.g. detecting person names, organizations, locations in texts\n",
        "\n",
        "## In this lab we will learn:\n",
        "1. Preprocessing: textual normalization, simple tokenization, stopword removal\n",
        "2. Converting documents into feature sets: Tf-Idf Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DVWV9PcLZDZD"
      },
      "source": [
        "---\n",
        "## Text Normalization and Preprocessing\n",
        "\n",
        "Text normalization is the process of transforming text into a single canonical form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AFNVhFanZDZF"
      },
      "source": [
        "### Lower case\n",
        "A computer does not **require** upper case letters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJWuPAkvZDZI",
        "outputId": "b89f3b3a-e481-4903-85c2-b7e28d1176d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# An example string:\n",
        "raw_1 = \"La pie niche haut. L’oie niche bas. Où l’hibou niche-t-il ?L’hibou niche ni haut ni bas. L'hibou niche pas.\"\n",
        "\n",
        "# Write code to lower case the string\n",
        "s = raw_1.lower()\n",
        "\n",
        "print(s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "la pie niche haut. l’oie niche bas. où l’hibou niche-t-il ?l’hibou niche ni haut ni bas. l'hibou niche pas.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ADYKKXo2ZDZP"
      },
      "source": [
        "### Handling Accented Characters \n",
        "\n",
        "Diacritics or accents on characters in English have a fairly marginal status, and we might well want `cliché` and `cliche` to match, or `naive` and `naïve`. This can be done by normalizing tokens to remove diacritics. In many other languages, diacritics are a regular part of the writing system and distinguish different sounds. Occasionally words are distinguished only by their accents. For instance, in French, `pêche` is `fishinig` while `péché` is `sin`.\n",
        "\n",
        "Nevertheless, the important question is usually not prescriptive or linguistic but is a question of how users are likely to write queries for these words. In many cases, users will enter queries for words without diacritics, whether for reasons of speed, laziness, limited software, or habits born of the days when it was hard to use non-ASCII text on many computer systems. In these cases, it might be best to equate all words to a form without diacritics.\n",
        "\n",
        "We will simply list all French accents and use string replace to convert accented characters with their canonical form.\n",
        "\n",
        "Let's replace accented characters with their canonical form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rucOEEZiZDZR",
        "colab": {}
      },
      "source": [
        "def normalize_accent(string):\n",
        "    string = string.replace('á', 'a')\n",
        "    string = string.replace('â', 'a')\n",
        "\n",
        "    string = string.replace('é', 'e')\n",
        "    string = string.replace('è', 'e')\n",
        "    string = string.replace('ê', 'e')\n",
        "    string = string.replace('ë', 'e')\n",
        "\n",
        "    string = string.replace('î', 'i')\n",
        "    string = string.replace('ï', 'i')\n",
        "\n",
        "    string = string.replace('ö', 'o')\n",
        "    string = string.replace('ô', 'o')\n",
        "    string = string.replace('ò', 'o')\n",
        "    string = string.replace('ó', 'o')\n",
        "\n",
        "    string = string.replace('ù', 'u')\n",
        "    string = string.replace('û', 'u')\n",
        "    string = string.replace('ü', 'u')\n",
        "\n",
        "    string = string.replace('ç', 'c')\n",
        "    \n",
        "    return string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0fcSPmzJZDZa",
        "outputId": "21098230-6e55-4e9a-f9f1-d25ed5e25e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(normalize_accent(s))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "la pie niche haut. l’oie niche bas. ou l’hibou niche-t-il ?l’hibou niche ni haut ni bas. l'hibou niche pas.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cONpho7iZDZj"
      },
      "source": [
        "### Tokenization : Spacy\n",
        "[spaCy](https://spacy.io/usage) is a platform to work with natural language data using Python.\n",
        "\n",
        "We will work with French data, so **you need to install proper the proper language package for French**. The complete installation instructions are [available in this link](https://spacy.io/usage).\n",
        "\n",
        "\n",
        "\n",
        "As usual, we will first convert everything to lowercase and normalize accents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iqFD8vspZDZn",
        "outputId": "5bbcc0a3-7c82-414c-faac-a2f02aa0e86e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "raw_2 = \"Latte ôtée, mur gâté, trou s’y fit, rat s’y mit, chat l’y vit, chat l’y prit.\"\n",
        "\n",
        "# Write code here to convert everything in lower case and to normalize accents.\n",
        "s = raw_2.lower()\n",
        "s = normalize_accent(s)\n",
        "\n",
        "print(s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "latte otee, mur gate, trou s’y fit, rat s’y mit, chat l’y vit, chat l’y prit.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sq7qw5v7ZDZw"
      },
      "source": [
        "`spaCy` already provides us with modules to easily tokenize the text. \n",
        "\n",
        "For that, first spaCy needs to be loaded with the required language. Then we can use the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kP3G83XNZDZx",
        "outputId": "4d072564-fdaa-4e75-cf44-4bc3fe95a310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "source": [
        "import spacy\n",
        "!python -m spacy download fr\n",
        "# Load spaCy for french\n",
        "spacy_nlp = spacy.load(\"fr\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fr_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7MB 624kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_sm==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.18.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (45.2.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: fr-core-news-sm\n",
            "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-cp36-none-any.whl size=14727027 sha256=d10de6d1e8ad3b585965247763a3221dd9598545e9f0155c179b910deaf04135\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xpcbyy2z/wheels/46/1b/e6/29b020e3f9420a24c3f463343afe5136aaaf955dbc9e46dfc5\n",
            "Successfully built fr-core-news-sm\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFGKjrgCZDZ1"
      },
      "source": [
        "[Here is a tutorial](https://towardsdatascience.com/a-short-introduction-to-nlp-in-python-with-spacy-d0aa819af3ad) for basic usages of spaCy.\n",
        "\n",
        "Upon running spaCy on a string, it automatically generates spaCy-token object. We simply need to get the string form (also called the orthogonal form) of the tokens. We will use `token.orth_` to get the string form of each token. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sljNaEUGZDZ2",
        "outputId": "dabfd0bd-7395-447c-de23-42071d25cbc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Tokenize\n",
        "spacy_tokens = spacy_nlp(s)\n",
        "string_tokens = [token.orth_ for token in spacy_tokens]\n",
        "\n",
        "print(string_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['la', 'pie', 'niche', 'haut', '.', 'l’', 'oie', 'niche', 'bas', '.', 'où', 'l’', 'hibou', 'niche', '-', 't', '-', 'il', '?', 'l’', 'hibou', 'niche', 'ni', 'haut', 'ni', 'bas', '.', \"l'\", 'hibou', 'niche', 'pas', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zbv8BrMeZDZ7"
      },
      "source": [
        "### Handling Punctuations\n",
        "\n",
        "Punctuations are noises while processing text. We are more interesting in the words itself.\n",
        "\n",
        "SpaCy recognises punctuation and is able to split these punctuation tokens from word tokens. We can use the `token.is_punct` to identify a punctuation token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XYocz9zQZDZ8",
        "outputId": "8b1e9cfa-551c-4096-f8d9-02571ba3fb77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Remove punctuation tokens\n",
        "string_tokens = [token.orth_ for token in spacy_tokens if not token.is_punct]\n",
        "\n",
        "print(string_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['la', 'pie', 'niche', 'haut', 'l’', 'oie', 'niche', 'bas', 'où', 'l’', 'hibou', 'niche', 't', 'il', 'l’', 'hibou', 'niche', 'ni', 'haut', 'ni', 'bas', \"l'\", 'hibou', 'niche', 'pas']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rcdx5e9NZDaC"
      },
      "source": [
        "### Stop word filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uEdRbe2PZDaE"
      },
      "source": [
        "Stop words are words which are filtered out before or after processing of natural language data (text). There is no universal stop-word list. Often, stop word lists include short function words, such as \"the\", \"is\", \"at\", \"which\", \"on\" etc. in English and \"le\", \"la\", \"et\", \"à\", \"qui\" etc. in French. Removing  stop-words has been shown to increase the performance of different tasks like search.\n",
        "\n",
        "Lucky for us, spaCy can also detect if a given token is a stop word. The stop words detection by spaCy will depend on the language we used to load spaCy. In our case it will detect only French stop words. \n",
        "\n",
        "We can use the `token.is_stop` to identify a stop word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yr7lWsbkZDaF",
        "outputId": "db174572-c3ae-400d-ed96-7c951bb1d4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Remove stop words\n",
        "string_tokens = [token.orth_ for token in spacy_tokens if not token.is_punct if not token.is_stop]\n",
        "\n",
        "print(string_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['pie', 'niche', 'haut', 'oie', 'niche', 'hibou', 'niche', 't', 'hibou', 'niche', 'haut', 'hibou', 'niche']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nns9QgeyZDaJ"
      },
      "source": [
        "### Lastly, recombining tokens into a string\n",
        "\n",
        "Many applications require to input a string, not a list of tokens. So we can merge the tokens into a single string. This will give us a **clean** string which we got after preprocessing of the raw string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DtId8eUlZDaL",
        "outputId": "6113a8cd-d5af-4209-8efe-1c3379adfb15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Combining list of tokens into a single string\n",
        "clean_2 = \" \".join(string_tokens)\n",
        "\n",
        "print(clean_2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "la pie niche haut . l’ oie niche bas . où l’ hibou niche - t - il ? l’ hibou niche ni haut ni bas . l' hibou niche pas .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d5qh1HmPZDaQ"
      },
      "source": [
        "## Let's combine everything: write a function\n",
        "Using above steps, we will now write a function. We will call this function **raw_to_text**. This function will take a raw text string and will return a list of tokens. We will also supply the spacy object for French which we loaded earlier. This will save us time to load the module again and again.\n",
        "1. lower case\n",
        "2. normalize accents (use the method we created before)\n",
        "3. tokenize\n",
        "4. remove punctuation tokens and stop words\n",
        "5. joining the tokens back into a single string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gdszweCPZDaR",
        "colab": {}
      },
      "source": [
        "def raw_to_tokens(raw_string, spacy_nlp):\n",
        "    # Write code for lower-casing\n",
        "    string = raw_string.lower()\n",
        "    \n",
        "    # Write code to normalize the accents\n",
        "    string = normalize_accent(string)\n",
        "        \n",
        "    # Write code to tokenize\n",
        "    spacy_tokens = spacy_nlp(string)\n",
        "        \n",
        "    # Write code to remove punctuation tokens and create string tokens\n",
        "    string_tokens = [token.orth_ for token in spacy_tokens if not token.is_punct if not token.is_stop]\n",
        "    \n",
        "    # Write code to join the tokens back into a single string\n",
        "    clean_string = \" \".join(string_tokens)\n",
        "    \n",
        "    return clean_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vkqWjvqlZDaV"
      },
      "source": [
        "Let's test the function with some sample data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vwd87syRZDaW",
        "outputId": "6aad2b8b-73fe-46db-8f9c-8328ab9033c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Raw string: \", raw_2)\n",
        "\n",
        "# Call the above raw_to_tokens function\n",
        "\n",
        "\n",
        "clean_2 = raw_to_tokens(raw_2, spacy_nlp)\n",
        "print(\"Clean string: \", clean_2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw string:  Latte ôtée, mur gâté, trou s’y fit, rat s’y mit, chat l’y vit, chat l’y prit.\n",
            "Clean string:  latte otee mur gate trou y fit rat y mit chat y vit chat y prit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wkEXNd0GZDaa"
      },
      "source": [
        "## Something bigger\n",
        "We will use [**EU Parliament Statements**](https://www.statmt.org/europarl/index.html) dataset. You have been supplied with a small extract of French statements for this lab. Here is an example:\n",
        ">Les critères de choix et les activités subventionnées dans le cadre de Leader atténuent, dans le meilleur des cas, une partie des problèmes de l'espace rural d' importance secondaire, tandis que dans le pire des cas, dégénèrent en un affaiblissement des relations publiques et une corruption des consciences.\n",
        "\n",
        "In this hands-on we will use 10,000 French documents extracted from the English-French bilingual dataset.\n",
        "\n",
        "The file **corpus.txt** supplied here, contains 10,000 documents. Each line of the file is a document.\n",
        "\n",
        "Now we will:\n",
        "   1. Load the documents as a list\n",
        "   2. Pre-process the documents\n",
        "   \n",
        "Note: Each line of the file **corpus.txt** is a document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f10achUXZDac",
        "outputId": "43068fee-45f1-4904-ac48-4c9e93dc73bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "Write code to load documents as a list\n",
        "\n",
        "Hint 1: open the file using open()\n",
        "Hint 2: use read() to load the content\n",
        "Hint 3: use splitlines() to get separate documents\n",
        "\n",
        "This will give us a list of strings, each string is document.\n",
        "\"\"\"\n",
        "\n",
        "file = open('./corpus.txt', 'r', encoding = 'utf-8')\n",
        "list = file.read()\n",
        "\n",
        "docs_raw = list.splitlines()\n",
        "\n",
        "print(\"Loaded \" + str(len(docs_raw)) + \" documents.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 10000 documents.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mh4L1TywZDai",
        "colab": {}
      },
      "source": [
        "a = []\n",
        "for i in range(len(docs_raw)):\n",
        "#for i in range(5):\n",
        "    clean_text = raw_to_tokens(docs_raw[i], spacy_nlp)\n",
        "    a.append(clean_text)\n",
        "\n",
        "docs_clean = a\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qw6P0a4YZDal",
        "outputId": "75175d9a-42df-4b8c-be13-1f2ace731b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(docs_clean)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W_p6S4wIZDap",
        "outputId": "ff260b86-7823-49c1-d17d-2197d4dcba1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Print sample documents\n",
        "print(\"Raw document: \", docs_raw[4])\n",
        "print(\"Preprocessed document: \", docs_clean[4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw document:  En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\n",
            "Preprocessed document:  attendant souhaiterais nombre collegues demande observions minute silence victimes tempetes pays union europeenne ete touches\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "224zsBLfZDat"
      },
      "source": [
        "## Vector Space Model\n",
        "We are interested in using this data to build statistical models. So, we now need to **vectorize** this data. The goal is to find a way to represent the data so that the computer can understand it.\n",
        "\n",
        "### Bag of words\n",
        "A bag of words represents each document in a corpus as a series of features. Most commonly, the features are the collection of all unique words in the vocabulary of the entire corpus. The values are usually the count of the number of times that word appears in the document, i.e. **term frequency**. \n",
        "\n",
        "A document $d$ is represented by a weight vector is $v_d=[w_{1,d} , w_{2,d},\\ldots, w_{N,d}]$ where $w_{t,d} = tf_{t,d}$, the term frequency of word $t$ in document $d$.\n",
        "\n",
        "A corpus is then represented as a matrix with one row per document and one column per unique word.\n",
        "\n",
        "### Scikit-Learn\n",
        "[Scikit-learn](http://scikit-learn.org/stable/) is machine learning library for the Python programming language. It features a wide range of machine learning algorithms for classification, regression and clustering. It also provides various supporting machine learning techniques such as cross validation, text vectorizer. Scikit-learn is designed to interoperate with the Python numerical and scientific libraries [NumPy](http://www.numpy.org/).\n",
        "\n",
        "Simple to use: import the required module and call it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MtTRz-VVZDay"
      },
      "source": [
        "## Vectorizer\n",
        "To build our initial bag of words count matrix, we will use scikit-learn's **CountVectorizer** class to transform our corpus into a bag of words representation. CountVectorizer expects as input a list of raw strings containing the documents in the corpus. It tabulates occurrance counts per document for each feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ndekKzMAZDa0",
        "outputId": "3eb48e35-02b1-42fe-9f0f-a8637e1e5e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "docs_raw_sample = [\"Chat vit rôt.\",\n",
        "                   \"Rôt tenta chat.\",\n",
        "                   \"Chat mit patte à rôt.\",\n",
        "                   \"Rôt trop chaud !\",\n",
        "                   \"Rôt brûla patte à chat.\",\n",
        "                   \"Chat quitta rôt.\"]\n",
        "\n",
        "# Write code to import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Write code to convert the list of documents to list of tokens.\n",
        "ls = []\n",
        "for i in range(len(docs_raw_sample)):\n",
        "    clean_text = raw_to_tokens(docs_raw_sample[i], spacy_nlp)\n",
        "    ls.append(clean_text)\n",
        "\n",
        "list_clean = ls\n",
        "\n",
        "\n",
        "# Write code to create a CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Write code to vectorize the sample text\n",
        "X_sample = vectorizer.fit_transform(list_clean)\n",
        "\n",
        "# The matrix is to be converted to dense matrix to print it\n",
        "print(\"Count Matrix:\")\n",
        "print(X_sample.todense())\n",
        "print(\"\\nWords in vocabulary:\")\n",
        "print(vectorizer.get_feature_names())\n",
        "print('\\n words counts:')\n",
        "print(len(vectorizer.get_feature_names()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count Matrix:\n",
            "[[0 1 0 0 0 0 1 0 1]\n",
            " [0 1 0 0 0 0 1 1 0]\n",
            " [0 1 0 1 1 0 1 0 0]\n",
            " [0 0 1 0 0 0 1 0 0]\n",
            " [1 1 0 0 1 0 1 0 0]\n",
            " [0 1 0 0 0 1 1 0 0]]\n",
            "\n",
            "Words in vocabulary:\n",
            "['brula', 'chat', 'chaud', 'mit', 'patte', 'quitta', 'rot', 'tenta', 'vit']\n",
            "\n",
            " words counts:\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qD13D3SjZDa6"
      },
      "source": [
        "## TF-IDF Weighting Scheme\n",
        "The tf-idf weighting scheme is an improvement over the simple term count or term frequency scheme we just saw. It is frequently used in text mining applications and has been shown to be effective. It combines two term statistics components:\n",
        "1. **Local component**: term count or term frequency (tf) reflects how important a word is to a document locally. For more details you can refer to [this link](https://nlp.stanford.edu/IR-book/html/htmledition/term-frequency-and-weighting-1.html).\n",
        "2. **Global component**: inverse document frequency (idf) of a word reflects how important the word is to the entire corpus or collection of documents. _Document frequency_ (df) of a word is the number of documents in the corpus where the word appears. A term with higher $df$ is a common term, thus carries less importance. $idf$ is an inverse function of $df$. So higher $idf$ means higher importance of the term globally. For more details you can refer to [this link](https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html).\n",
        "\n",
        "The weight vector for document $d$ under tf-idf scheme is $v_d=[w_{1,d} , w_{2,d},\\ldots, w_{N,d}]$ where $w_{t,d}=tf_{t,d}\\times\\log\\frac{Card(D)}{Card(d'\\in D | t\\in d') + 1}$ In the denominator we have added 1 to avoid division by zero, which is called smoothing. \n",
        "\n",
        "Scikit-learn has your back, it already provides the **TfidfVectorizer** module to compute TF-IDF matrix.\n",
        "\n",
        "**Note**:\n",
        "1. Scikit-learn uses a slightly different formula than that we saw today morning. You can refer to [corresponding documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) to know more.\n",
        "2. Do not forget about preprocessing and tokenization before doing vectorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MHrg6TmmZDa7",
        "outputId": "58f40636-4da6-489c-bc34-0634c1fe224e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "docs_raw_sample = [\"Chat vit rôt.\",\n",
        "                   \"Rôt tenta chat.\",\n",
        "                   \"Chat mit patte à rôt.\",\n",
        "                   \"Rôt trop chaud !\",\n",
        "                   \"Rôt brûla patte à chat.\",\n",
        "                   \"Chat quitta rôt.\"]\n",
        "\n",
        "# Write code to import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Write code to convert the list of documents to list of tokens.\n",
        "ls = []\n",
        "for i in range(len(docs_raw_sample)):\n",
        "    clean_text = raw_to_tokens(docs_raw_sample[i], spacy_nlp)\n",
        "    ls.append(clean_text)\n",
        "\n",
        "list_clean = ls\n",
        "\n",
        "# Write code to create a TfidfVectorizer object\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# Write code to vectorize the sample text\n",
        "X_tfidf_sample = tfidf.fit_transform(list_clean)\n",
        "\n",
        "print(\"Shape of the TF-IDF Matrix:\")\n",
        "print(X_tfidf_sample.shape)\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(X_tfidf_sample.todense())\n",
        "print(tfidf.get_feature_names())\n",
        "\n",
        "# 有一些为0的是因为tf = 0，也就是这个词在第i篇文章里没出现过"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the TF-IDF Matrix:\n",
            "(6, 9)\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.42407356 0.         0.         0.         0.\n",
            "  0.36743345 0.         0.82774046]\n",
            " [0.         0.42407356 0.         0.         0.         0.\n",
            "  0.36743345 0.82774046 0.        ]\n",
            " [0.         0.35088001 0.         0.68487548 0.56160769 0.\n",
            "  0.30401578 0.         0.        ]\n",
            " [0.         0.         0.91399636 0.         0.         0.\n",
            "  0.40572238 0.         0.        ]\n",
            " [0.68487548 0.35088001 0.         0.         0.56160769 0.\n",
            "  0.30401578 0.         0.        ]\n",
            " [0.         0.42407356 0.         0.         0.         0.82774046\n",
            "  0.36743345 0.         0.        ]]\n",
            "['brula', 'chat', 'chaud', 'mit', 'patte', 'quitta', 'rot', 'tenta', 'vit']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H9iYR3CBZDbA"
      },
      "source": [
        "## From Documents to Features\n",
        "TF-IDF basically transforms a set of documents into a set of features, which can be directly used in machine learning tasks.\n",
        "\n",
        "Let's now convert EU Parliamanet documents into Tf-Idf vectors.\n",
        "\n",
        "A correct conversion will generate a matrix of dimensions (10000, 13429)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KB5Je9yhZDbB",
        "outputId": "53eb4d78-d0fb-4e9b-85cf-10f8e5ecbedd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Write code to convert raw documents into TF-IDF matrix.\n",
        "\"\"\"\n",
        "Hint: - create a TfidfVectorizer for clean EU Parliamanet documents\n",
        "      - use fit_transform to vectorize raw_docs\n",
        "\"\"\"\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(docs_clean)\n",
        "\n",
        "print(\"Shape of the TF-IDF Matrix:\")\n",
        "print(X_tfidf.shape)\n",
        "# 10000 documents with 13427 words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the TF-IDF Matrix:\n",
            "(10000, 13429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JKejXBHoZDbE"
      },
      "source": [
        "## Course Project: Text Classification with Rakuten France Product Data\n",
        "\n",
        "The project focuses on the topic of large-scale product type code text classification where the goal is to predict each product’s type code as defined in the catalog of Rakuten France. This project is derived from a data challenge proposed by Rakuten Institute of Technology, Paris. Details of the data challenge is [available in this link](https://challengedata.ens.fr/challenges/35).\n",
        "\n",
        "The above data challenge focuses on multimodal product type code classification using text and image data. **For this project we will work with only text part of the data.**\n",
        "\n",
        "Please read carefully the description of the challenge provided in the above link. **You can disregard any information related to the image part of the data.**\n",
        "\n",
        "### To obtain the data\n",
        "You have to register yourself [in this link](https://challengedata.ens.fr/challenges/35) to get access to the data.\n",
        "\n",
        "For this project you will only need the text data. Download the training files `x_train` and `y_train`, containing the item texts, and the corresponding product type code labels.\n",
        "\n",
        "### Pandas for handling the data\n",
        "The files you obtained are in CSV format. We strongly suggest to use Python Pandas package to load and visualize the data. [Here is a basic tutorial](https://data36.com/pandas-tutorial-1-basics-reading-data-files-dataframes-data-selection/) on how to handle data in CSV file using Pandas.\n",
        "\n",
        "If you open the `x_train` dataset using Pandas, you will find that it contains following columns:\n",
        "1. an integer ID for the product\n",
        "2. **designation** - The product title\n",
        "3. description\n",
        "4. productid\n",
        "5. imageid\n",
        "\n",
        "For this project we will only need the integer ID and the designation. You can [`drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) the other columns.\n",
        "\n",
        "The training output file `y_train.csv` contains the **prdtypecode**, the target/output variable for the classification task, for each integer id in the training input file `X_train.csv`.\n",
        "\n",
        "### Task for the break\n",
        "1. Register yourself and download the training and test for text data. You do not need the `supplementary files` for this project.\n",
        "2. Load the data using pandas and disregard unnecessary columns as mentioned above.\n",
        "3. On the **designation** column, apply the preprocessing techniques.\n",
        "\n",
        "### Task for the end of the course\n",
        "After this preprocessing step, you have now access to a TF-IDF matrix that constitute our data set for the final evaluation project. The project guidelines are:\n",
        "1. Apply all approaches taught in the course and practiced in lab sessions (Decision Trees, Bagging, Random forests, Boosting, Gradient Boosted Trees, AdaBoost, etc.) on this data set. The goal is to predict the target variable (prdtypecode).\n",
        "2. Compare performances of all these models in terms of the weighted-f1 scores you can output. \n",
        "3. Conclude about the most appropriate approach on this data set for the predictive task. \n",
        "4. Write a report in .tex format that adress all these guidelines with a maximal page number of 5 (including figures, tables and references). We will take into account the quality of writing and presentation of the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_LkesKjKZDbF",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('X_train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gWRWJikpfaqj",
        "outputId": "4e933863-b181-4b94-ce96-e796f16c270b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n",
            "Wed Mar 18 17:18:43 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.59       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ELNfhLtcZDbJ",
        "outputId": "a17e98a9-6722-42df-eead-f0cb913eb901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>designation</th>\n",
              "      <th>description</th>\n",
              "      <th>productid</th>\n",
              "      <th>imageid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3804725264</td>\n",
              "      <td>1263597046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>436067568</td>\n",
              "      <td>1008141237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
              "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
              "      <td>201115110</td>\n",
              "      <td>938777978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50418756</td>\n",
              "      <td>457047496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>La Guerre Des Tuques</td>\n",
              "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
              "      <td>278535884</td>\n",
              "      <td>1077757786</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...     imageid\n",
              "0           0  ...  1263597046\n",
              "1           1  ...  1008141237\n",
              "2           2  ...   938777978\n",
              "3           3  ...   457047496\n",
              "4           4  ...  1077757786\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ux_4n5rKZDbM",
        "colab": {}
      },
      "source": [
        "docs_raw = train['designation']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WCmff3QbZDbP",
        "outputId": "7a2a8c5e-44b3-4ab5-eb33-34b292c1966a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "docs_raw"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Olivia: Personalisiertes Notizbuch / 150 Seite...\n",
              "1        Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...\n",
              "2        Grand Stylet Ergonomique Bleu Gamepad Nintendo...\n",
              "3        Peluche Donald - Europe - Disneyland 2000 (Mar...\n",
              "4                                     La Guerre Des Tuques\n",
              "                               ...                        \n",
              "84911                          The Sims [ Import Anglais ]\n",
              "84912    Kit piscine acier NEVADA déco pierre Ø 3.50m x...\n",
              "84913    Journal Officiel De La Republique Francaise N°...\n",
              "84914    Table Basse Bois De Récupération Massif Base B...\n",
              "84915    Gomme De Collection 2 Gommes Pinguin Glace Ver...\n",
              "Name: designation, Length: 84916, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LLd4gVf4ZDbT",
        "colab": {}
      },
      "source": [
        "a = []\n",
        "for i in range(len(docs_raw)):\n",
        "#for i in range(5):\n",
        "    clean_text = raw_to_tokens(docs_raw[i], spacy_nlp)\n",
        "    a.append(clean_text)\n",
        "\n",
        "docs_clean = a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZdRGmGRPZDbX",
        "outputId": "10a53d40-4355-454f-d631-39464a60a95a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(docs_clean)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84916"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYDWxyJgsjej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b6f2078-9fb6-41aa-9197-078b0047140c"
      },
      "source": [
        "test = pd.read_csv('X_test_update.csv')\n",
        "docs_raw_2 = test['designation']\n",
        "a = []\n",
        "for i in range(len(docs_raw_2)):\n",
        "#for i in range(5):\n",
        "    clean_text = raw_to_tokens(docs_raw_2[i], spacy_nlp)\n",
        "    a.append(clean_text)\n",
        "\n",
        "docs_clean_2 = a\n",
        "len(docs_clean_2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13812"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIl991ewwr3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbfefd62-04fc-496e-a74e-37d17944b426"
      },
      "source": [
        "final_sample = docs_clean + docs_clean_2\n",
        "len(final_sample)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98728"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YRJnw-iHZDba",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Write code to create a CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Write code to vectorize the sample text\n",
        "X_sample = vectorizer.fit_transform(final_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O_vIcDMwZDbe",
        "outputId": "87ef223e-de81-493f-d5c1-e21423ddac6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_sample.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98728, 86833)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gQIYqxtesw44",
        "colab": {}
      },
      "source": [
        "Y_sample_train = pd.read_csv('Y_train.csv', index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ty01jWous525",
        "outputId": "6d87c870-7049-4450-8227-67fb93e4bd10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_sample_train.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(84916, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzimWOYuxrsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3b204205-bf78-40ea-885b-e692f69eac04"
      },
      "source": [
        "X_sample[:len(docs_clean)]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<84916x86833 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 745492 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QftPfLl1s55p",
        "outputId": "c7d3f992-73c9-4340-81be-3437931e63fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier  # use decision tree as base estimator\n",
        "bagging = BaggingClassifier(None, 20, 0.5, 0.5)\n",
        "\n",
        "bagging.fit(X_sample[:len(docs_clean)], Y_sample_train)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_bagging.py:645: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
              "                  max_features=0.5, max_samples=0.5, n_estimators=20,\n",
              "                  n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
              "                  warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StEtSHTKz57s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_original = pd.DataFrame(bagging.predict(X_sample[len(docs_clean):]), columns=['prdtypecode'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RATbqmck48h4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_original.index += 84916"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjUmcqru3gOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_original.to_csv('pre_original.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ouyW48uys58J",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5K-ZsrSXs5-e",
        "outputId": "e4d71f9f-4b1c-40c7-bc21-135fbf9b2b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "result_rf = cross_val_score(bagging, X_sample, Y_sample, cv=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_bagging.py:645: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_bagging.py:645: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_bagging.py:645: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_bagging.py:645: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_bagging.py:645: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1_E_2GMM74Tf",
        "outputId": "b633ee90-a6b8-414e-de29-267d19b294a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result_rf.mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7731875741247883"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    }
  ]
}